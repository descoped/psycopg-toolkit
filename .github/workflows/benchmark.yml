name: Performance Benchmarks

on:
  workflow_dispatch:
    inputs:
      run_extended:
        description: 'Run extended performance tests (takes longer)'
        required: false
        type: boolean
        default: true
  pull_request:
    branches:
      - master
    paths:
      - 'src/psycopg_toolkit/repositories/**'
      - 'src/psycopg_toolkit/utils/json_handler.py'
      - 'src/psycopg_toolkit/utils/type_inspector.py'
      - 'tests/performance/**'

env:
  PYTHON_VERSION: '3.13'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v6

    - name: Install dependencies
      run: |
        uv sync --all-groups
    
    # Schema setup handled by test fixtures
    
    - name: Run performance benchmarks
      run: |
        # Run performance tests with output capture
        echo "Running standard performance tests..."
        uv run pytest tests/performance/test_jsonb_performance.py -m performance -v --tb=short -s | tee benchmark-output.txt
        
        # Run extended performance tests if requested
        if [[ "${{ github.event.inputs.run_extended }}" == "true" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          echo -e "\n\nRunning extended performance tests..."
          uv run pytest tests/performance/test_jsonb_performance_extended.py -m performance -v --tb=short -s | tee -a benchmark-output.txt
        fi
    
    - name: Generate benchmark summary
      if: always()
      run: |
        echo "## ðŸ“Š Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Extract performance metrics from output
        if [ -f benchmark-output.txt ]; then
          echo "### Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Extract lines containing performance data (handles both standard and extended format)
          grep -E "===|avg=|total=|per record=|overhead:|Avg:|Min:|Max:|Median:|StdDev:|Throughput:|Results:|Batch|JSONB:|Regular:|insert:|query:|update:" benchmark-output.txt | grep -v "test session starts" | grep -v "passed," | grep -v "collected" >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Test summary
          echo "### Test Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          grep -E "(PASSED|FAILED|test_)" benchmark-output.txt | tail -20 >> $GITHUB_STEP_SUMMARY || true
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Metadata" >> $GITHUB_STEP_SUMMARY
        echo "- **Run Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "- **Python Version**: ${{ env.PYTHON_VERSION }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Workflow Run**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
    
    - name: Process benchmark results
      if: always()
      run: |
        # Create a consolidated results file
        echo "# Performance Benchmark Report" > benchmark-results.md
        echo "Generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> benchmark-results.md
        echo "" >> benchmark-results.md
        
        if [ -f benchmark-output.txt ]; then
          echo "## Raw Output" >> benchmark-results.md
          echo '```' >> benchmark-results.md
          cat benchmark-output.txt >> benchmark-results.md
          echo '```' >> benchmark-results.md
        fi
    
    - name: Upload benchmark artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.run_id }}
        path: |
          benchmark-output.txt
          benchmark-results.md
        retention-days: 30
        if-no-files-found: warn